----------------- 特征工程 -------------------
特征工程的核心，是基于现有特征，衍生，组合并筛选出强特征的过程

第一步：特征衍生
常用方法有：
1）字段拼接，形成新字段
2）基于常识或者业务规则的打标
3）分类聚合、分类计数
4）分箱处理
5）关联历史数据(平均值、大多数值)，填充缺失值
6）编码

第二步：特征筛选
1）特征评估，常用参数有覆盖率、缺失率、零值率、AUC、KS、PSI（评估两个时间段特征分布差异的度量）、IV（评估好坏用户分布差异的度量）
2）特征关联性检测，删除类似或者一样的特征
3）删除明显不符合常识和业务的特征
4）删除高缺失率，低IV的特征
5）多特征筛选（将单特征加上一些常规特征合并训练，以此剔除无用的特征）
常规特征包括
星座特征: 最没用的特征，剔除小于星座特征的重要程度的特征
Boruta: 将原始数据随机打乱，然后合并一起训练，比较原始和打乱后的重要程度（RandomForest,lightgbm,xgboost中的包含的feature_importances_），提出原始重要程度不如打乱后的数据
方差膨胀系数(VIF_ev)：一个特征是其他一组特征的线性组合，去掉
REF递归特征消除：把模型性能下降最少的特征去掉
----
基于L1（L1正则化）的特征选择：线性回归+ L1正则化 = 
线性回归: 利用回归方程、函数对一个或多个自变量(特征值)和因变量(目标值)之间关系进行建模的一种分析方式
损失函数(描述真实结果与预测结果差异的)， 损失函数有多种：例如误差的平方
L2正则化（ridge regression 岭回归）：线性回归的系数(θ)求平方然后累加起来+原来的损失函数 = 新的损失函数
L1正则化（Lasso regression Lasso回归）：线性回归的系数(θ)绝对值累加起来+原来的损失函数 = 新的损失函数
L1正则化和L2正则化的区别：L1正则化将不重要的系数变为0，L2正则化将不重要的系数变为的很小

梯度下降算法？：沿着梯度的反方向更新模型参数
----

第三步：模型训练
第四步：模型上线
第五步：模型监控
1）内部特征监控，放款前特征报表监控(缺失率、零值率、PSI)，例如：数据缺失率逐渐稳定，某个数据趋势率突然攀升；放款后特征报表监控(AUC/KS)；分箱比例监控
2）外部特征评估，外部数据的报表监控（缺失率、零值率、覆盖率(交集/内部，如果付费，切换到内部数据，)、PSI、AUC/KS），提高模型准确率，避免未来信息(上线后时间验证)，避免内部数据泄露(hash加密)，避免三方公司对结果美化(加入假样本)
3）混淆矩阵、查准率tp/tp + fp（预测结果为正中真实正例的比例）、召回率（真实为正例中，预测结果为正的比例）tp / tp + FN，F1-score（精确率和正确率的调和平均）
4）roc曲线，横坐标FPR假阳性率（fp/fp+tn）,纵坐标TPF真阳性率（TP/TP+FN）,roc曲线中最理想的点为(0,1)此时，fp=fn=0，auc就是ROC曲线的面积