# 1. 数据理解

## 1.1 下载数据

下载地址：https://tianchi.aliyun.com/competition/entrance/531994/information



## 1.2 背景理解

赛题以保险风控为背景，保险是重要的金融体系，对社会发展，民生保障起到重要作用。保险欺诈近些年层出不穷，在某些险种上保险欺诈的金额已经占到了理赔金额的20%甚至更多。对保险欺诈的识别成为保险行业中的关键应用场景。



数据集提供了之前客户索赔的车险数据，希望你能开发模型帮助公司预测哪些索赔是欺诈行为
To DO：预测用户的车险是否为欺诈行为



|            字段             |                    说明                     |
| :-------------------------: | :-----------------------------------------: |
|          policy_id          |                  保险编号                   |
|             age             |                    年龄                     |
|       customer_months       |         成为客户的时长，以月为单位          |
|      policy_bind_date       |                保险绑定日期                 |
|        policy_state         |               上保险所在地区                |
|         policy_csl          |      组合单一限制Combined Single Limit      |
|      policy_deductable      |                 保险扣除额                  |
|    policy_annual_premium    |                 每年的保费                  |
|       umbrella_limit        |                保险责任上限                 |
|         insured_zip         |                 被保人邮编                  |
|         insured_sex         |         被保人姓名：FEMALE或者MALE          |
|   insured_education_level   |                 被保人学历                  |
|     insured_occupation      |                 被保人职业                  |
|       insured_hobbies       |               被保人兴趣爱好                |
|    insured_relationship     |                 被保人关系                  |
|        capital-gains        |                  资本收益                   |
|        capital-loss         |                  资本损失                   |
|        incident_date        |                  出险日期                   |
|        incident_type        |                  出险类型                   |
|       collision_type        |                  碰撞类型                   |
|      incident_severity      |                事故严重程度                 |
|    authorities_contacted    |            联系了当地的哪个机构             |
|       incident_state        |           出事所在的省份，已脱敏            |
|        incident_city        |           出事所在的城市，已脱敏            |
|  incident_hour_of_the_day   |   出事所在的小时（一天24小时的哪个时间）    |
| number_of_vehicles_involved |                涉及的车辆数                 |
|       property_damage       |               是否有财产损失                |
|       bodily_injuries       |                  身体伤害                   |
|          witnesses          |                  目击证人                   |
|   police_report_available   |            是否有警察记录的报告             |
|     total_claim_amount      |                整体索赔金额                 |
|        injury_claim         |                伤害索赔金额                 |
|       property_claim        |                财产索赔金额                 |
|        vehicle_claim        |                汽车索赔金额                 |
|          auto_make          | 汽车品牌，比如Audi, BMW, Toyota, Volkswagen |
|         auto_model          |     汽车型号，比如A3,X5,Camry,Passat等      |
|          auto_year          |               汽车购买的年份                |
|            fraud            |              是否欺诈，1或者0               |

评价标准： AUC, 即ROC曲线下面的面积 (Area under the Curve of ROC)



通过以上数据，进行是否存在保险欺诈的预测，是二分类问题，评价标准AUC值



## 1.3 数据概况

```
### 一、数据概况分析的维度

1. 数据质量
   - **缺失值**：检查数据中是否存在缺失值，以及缺失值的比例和分布。
   - **异常值**：识别数据中的异常值，这些值可能是由于测量误差、数据录入错误或特殊事件导致的。
   - **重复值**：检查数据中是否存在重复记录，重复值可能会影响模型的准确性和效率。
2. 数据分布
   - **数值型数据**：了解数值型数据的分布形态，如正态分布、偏态分布等。
   - **类别型数据**：分析类别型数据的频数分布，了解各类别的占比情况。
3. 数据相关性
   - **特征间相关性**：计算特征之间的相关系数，识别高度相关的特征，这有助于特征选择和降维。
   - **目标变量相关性**：分析特征与目标变量之间的相关性，了解哪些特征对目标变量有重要影响。
4. 数据规模与维度
   - **样本量**：了解数据集的样本量大小，样本量过小可能导致模型过拟合。
   - **特征维度**：分析数据集的特征维度，高维度数据可能需要降维处理。

### 二、帮助分析数据整体情况的图形

1. 缺失值分析
   - **缺失值矩阵图**：使用热力图或条形图展示各特征的缺失值情况，直观了解缺失值的分布。
2. 数据分布分析
   - **直方图**：用于展示数值型数据的分布形态，如正态分布、偏态分布等。
   - **核密度图（KDE）**：与直方图类似，但更平滑，能够更准确地反映数据的分布形态。
   - **箱线图**：用于展示数据的五数概要（最小值、第一四分位数、中位数、第三四分位数、最大值），以及识别异常值。
   - **条形图/饼图**：用于展示类别型数据的频数分布。
3. 数据相关性分析
   - **相关系数矩阵图**：使用热力图展示特征之间的相关系数，直观了解特征间的相关性。
   - **散点图矩阵**：用于展示特征之间的两两关系，有助于发现特征间的线性或非线性关系。
4. 数据规模与维度分析
   - **特征重要性排序图**：在模型训练后，可以使用条形图或折线图展示特征的重要性排序，了解哪些特征对模型有重要影响。
   - **降维可视化**：如使用PCA（主成分分析）或t-SNE等方法对数据进行降维处理，并使用散点图展示降维后的数据分布，有助于理解数据的高维结构和内在规律。
```



![image-20250415175354002](https://gitee.com/fubob/note-pic/raw/master/image/image-20250415175354002.png)



# 2. 特征工程

```
在机器学习中，编码方式是将类别型数据（非数值型数据）转换为数值型数据的过程，以便算法能够处理。以下是常用的编码方式及其优缺点：

### 1. **Label Encoding（标签编码）**

- **描述**：将每个类别标签映射为一个整数。例如，`['红色', '蓝色', '绿色']`可以编码为`[0, 1, 2]`。
- **优点**：
  - 简单易用，实现方便。
  - 保留了类别之间的某种顺序关系（如果有的话），尽管这种顺序在大多数情况下是任意的。
- **缺点**：
  - 可能会引入不必要的顺序关系，导致算法误解类别之间的相对重要性。
  - 对于没有内在顺序的类别数据，标签编码可能不是最佳选择。

### 2. **One-Hot Encoding（独热编码）**

- **描述**：为每个类别创建一个二进制列，只有对应类别的列值为1，其余为0。例如，`['红色', '蓝色', '绿色']`可以编码为三列：`[1, 0, 0]`、`[0, 1, 0]`、`[0, 0, 1]`。
- **优点**：
  - 避免了引入虚假的顺序关系。
  - 适用于大多数机器学习算法，尤其是那些对特征之间独立性有要求的算法。
- **缺点**：
  - 当类别数量很多时，会导致特征维度急剧增加，造成“维度灾难”。
  - 可能会产生稀疏矩阵，增加存储和计算成本。

### 3. **Ordinal Encoding（序数编码）**

- **描述**：类似于标签编码，但更侧重于为具有自然顺序的类别分配整数。例如，教育程度可以编码为`['小学', '初中', '高中', '大学']`对应`[1, 2, 3, 4]`。
- **优点**：
  - 适用于具有明确顺序的类别数据。
  - 比标签编码更明确地表达了类别之间的顺序关系。
- **缺点**：
  - 如果类别之间没有明确的顺序，使用序数编码可能会误导算法。
  - 仍然可能引入不必要的数值关系。

### 4. **Binary Encoding（二进制编码）**

- **描述**：将每个类别标签转换为二进制字符串，然后将这些二进制字符串拆分为单独的二进制位作为特征。例如，如果有4个类别，可以用2位二进制数表示（`00`, `01`, `10`, `11`）。
- **优点**：
  - 在一定程度上减少了特征维度，相比独热编码更节省空间。
  - 保留了类别之间的部分差异信息。
- **缺点**：
  - 编码后的特征之间可能存在相关性，这可能会影响某些算法的性能。
  - 二进制编码的解读性不如独热编码直观。

### 5. **Target Encoding（目标编码）**

- **描述**：使用目标变量的统计信息（如均值、中位数等）来编码类别特征。例如，对于每个类别，计算该类别下目标变量的平均值，并用这个平均值来代表该类别。
- **优点**：
  - 能够捕捉类别与目标变量之间的关系。
  - 在某些情况下，可以显著提高模型的预测性能。
- **缺点**：
  - 可能会引入数据泄露的风险，尤其是在训练集和测试集划分不当的情况下。
  - 需要谨慎处理以避免过拟合。

### 6. **Frequency Encoding（频率编码）**

- **描述**：将每个类别映射为该类别在数据集中出现的频率。
- **优点**：
  - 简单易用，能够反映类别在数据集中的分布情况。
  - 在某些情况下，频率信息可能对模型有用。
- **缺点**：
  - 可能会丢失类别之间的其他重要信息。
  - 对于类别分布极不均衡的数据集，频率编码可能不是最佳选择。

### 7. **Embedding Encoding（嵌入编码）**

- **描述**：通常用于深度学习模型中，将高维的类别特征映射到低维的连续向量空间中。这些向量在训练过程中学习得到，能够捕捉类别之间的复杂关系。
- **优点**：
  - 能够处理高基数（类别数量很多）的类别特征。
  - 在深度学习模型中，嵌入编码通常能够取得更好的性能。
- **缺点**：
  - 需要大量的数据来训练有效的嵌入向量。
  - 嵌入向量的解释性较差，难以理解其具体含义。

### 总结

- 选择编码方式时需要考虑的因素：
  - 类别特征的性质（是否有序、基数大小等）。
  - 模型的类型和要求（如是否对特征独立性有要求）。
  - 数据的分布和特性（如类别分布是否均衡）。
- 常用编码方式的适用场景：
  - 对于无序且基数较小的类别特征，独热编码是常用的选择。
  - 对于有序类别特征，序数编码或目标编码可能更合适。
  - 对于高基数的类别特征，嵌入编码或频率编码可能更有效。
```



## 2.1 空值填充

```
X_train = X_train.fillna({'authorities_contacted': 'no_contacted'})
X_train['authorities_contacted'].isnull().sum()
```



## 2.2 对日期进行编码

```
针对日期进行编码是机器学习和数据处理中的一个常见任务，因为日期数据通常包含丰富的信息，如时间顺序、周期性等。以下是几种常用的日期编码方法：

1. 提取日期特征
将日期分解为年、月、日、星期几、季度等特征，然后分别对这些特征进行编码。

年（Year）：直接作为数值特征。
月（Month）：可以编码为1到12的整数，或者使用独热编码（One-Hot Encoding）转换为12个二进制特征。
日（Day）：可以编码为1到31的整数，但考虑到月份天数不同，通常也使用独热编码或保留为数值。
星期几（Weekday）：可以编码为0到6的整数（通常0代表星期一），或者使用独热编码。
季度（Quarter）：可以编码为1到4的整数，或者使用独热编码。
示例：

python
import pandas as pd
 
# 创建一个示例日期列
df = pd.DataFrame({'date': pd.date_range(start='2023-01-01', periods=5, freq='D')})
 
# 提取日期特征
df['year'] = df['date'].dt.year
df['month'] = df['date'].dt.month
df['day'] = df['date'].dt.day
df['weekday'] = df['date'].dt.weekday
df['quarter'] = df['date'].dt.quarter
 
# 对月份进行独热编码（示例）
df = pd.get_dummies(df, columns=['month'], prefix='month')
 
print(df)

2. 时间戳（Timestamp）或序数（Ordinal）编码
将日期转换为时间戳（自某个固定日期以来的秒数或天数）或序数（如年份内的第几天），然后作为数值特征使用。

时间戳：适用于需要保留日期之间精确时间差的情况。
序数：如年份内的第几天，可以捕捉日期在一年中的位置。
示例：

python
# 将日期转换为时间戳（以天为单位）
df['timestamp'] = (df['date'] - pd.Timestamp("1970-01-01")).dt.days
 
# 将日期转换为年份内的第几天
df['day_of_year'] = df['date'].dt.dayofyear
 
print(df[['timestamp', 'day_of_year']])

3. 周期性特征编码
对于具有周期性的日期特征（如月份、星期几），可以使用正弦和余弦函数将其转换为周期性特征。这种方法可以捕捉日期特征的周期性变化。

示例（以月份为例）：

python
import numpy as np
 
# 将月份转换为周期性特征
df['month_sin'] = np.sin(2 * np.pi * df['month_1'] / 12)  # 假设month_1是独热编码后的第一个月份特征（实际应使用所有月份特征前先转换）
# 更正：由于独热编码后不能直接用于此公式，这里仅为说明周期性编码概念。
# 实际应用中，若要用周期性编码月份，应直接使用月份数值（1-12）
df['month_actual'] = df['date'].dt.month  # 重新获取月份数值
df['month_sin'] = np.sin(2 * np.pi * df['month_actual'] / 12)
df['month_cos'] = np.cos(2 * np.pi * df['month_actual'] / 12)
 
# 注意：上面的month_1仅为示意，实际应直接使用月份数值进行周期性编码
print(df[['month_actual', 'month_sin', 'month_cos']])
注意：在实际应用中，应直接使用月份数值（1-12）进行周期性编码，而不是独热编码后的特征。

4. 自定义编码
根据具体问题的需求，可以设计自定义的编码方案。例如，对于某些特定节日或事件，可以创建二进制特征来指示这些日期的存在。

总结
提取日期特征：将日期分解为年、月、日等特征，然后分别编码。
时间戳或序数编码：将日期转换为时间戳或序数，作为数值特征使用。
周期性特征编码：使用正弦和余弦函数将周期性日期特征转换为连续值。
自定义编码：根据具体需求设计自定义编码方案。
选择哪种编码方法取决于具体问题的需求和数据的特性。在实际应用中，可能需要结合多种编码方法来充分捕捉日期数据中的信息。
```



```python
# 这里的数据，最好是转换为有序编码
def date_to_timestamp_days(date_series):
    """
    将日期序列转换为以天为单位的时间戳（自1970年1月1日起）。

    参数:
    date_series (pd.Series): 包含日期的Pandas Series对象。

    返回:
    pd.Series: 包含时间戳（以天为单位）的Pandas Series对象。
    """
    
    # 确保输入是datetime类型

    if date_series.dtype == 'object':
        try:
            date_series_convert = pd.to_datetime(date_series, format='%Y-%m-%d')
        except Exception as e:
            print(f"转换为datetime类型时出错: {e}")
    else:
        print("'已经是非object类型，无需转换。")

    if not pd.api.types.is_datetime64_any_dtype(date_series_convert):
        raise ValueError("输入的Series必须是datetime类型")

    # 计算时间戳（以天为单位）
    timestamp_days = (date_series_convert - pd.Timestamp("1970-01-01")).dt.days
    return timestamp_days

X_train['incident_date_timestamp_days'] = date_to_timestamp_days(X_train['incident_date']) 
X_train['policy_bind_date_timestamp_days'] = date_to_timestamp_days(X_train['policy_bind_date']) 
```



## 2.2 object类型转换成编码

```python
# 4.除了日期，剩余object都是独立的类别，重复都在10以下，除了insured_occupation 14， insured_hobbies 20 ，auto_make 14， auto_model 39
# 这里我选择独热编码，避免标签编码出现的有序，给模型带来错觉

# 3.2 独热编码，注意两点，一是避免重复使用同一encoder实例fit，二是OneHotEncoder需要二维数组作为输入，因此我们需要对单个列进行reshape

object_feature = list(X_train.select_dtypes(include=['object']).columns)

for col in object_feature:
    # 正确的单独处理示例（避免重复使用同一encoder实例fit）：
    onehot = OneHotEncoder(sparse_output=False, drop='first')
    if col not in ['incident_date','policy_bind_date']:
        # 注意：OneHotEncoder需要二维数组作为输入，因此我们需要对单个列进行reshape
        X_train[col] = onehot.fit_transform(X_train[[col]])
X_train
```



# 3. 数据清洗

```
# 数据id是人的唯一代码，从常识，业务上应该直接删除
for col in ['incident_date','age']: # ,
    del x_train[col]
    del x_test[col]
```





# 3. 模型训练

## 3.1 模型选型

```
在机器学习中，有多种常用且效果较好的模型，它们在不同的任务和数据集上表现出色。以下是一些最常用的机器学习模型及其简要介绍：

1. 线性回归（Linear Regression）
用途：用于预测连续值，如房价、股票价格等。
原理：通过最小化预测值与真实值之间的均方误差来拟合数据。
优点：简单易懂，计算效率高，适用于线性关系明显的数据。
2. 逻辑回归（Logistic Regression）
用途：用于二分类问题，如垃圾邮件检测、疾病诊断等。
原理：通过逻辑函数将线性回归的输出映射到概率值，用于分类。
优点：计算效率高，易于实现，输出概率值便于解释。
3. 决策树（Decision Trees）
用途：用于分类和回归问题。
原理：通过递归地划分数据集来构建树形结构，每个内部节点表示一个特征上的测试，每个分支表示测试输出，每个叶节点表示一个类别或值。
优点：易于理解和解释，能够处理非线性关系。
4. 随机森林（Random Forests）
用途：用于分类和回归问题。
原理：通过构建多个决策树并结合它们的输出来提高模型的准确性和稳定性。
优点：能够处理高维数据，对缺失值和异常值具有鲁棒性，通常比单个决策树表现更好。
5. 支持向量机（Support Vector Machines, SVM）
用途：用于分类和回归问题，尤其在二分类问题上表现出色。
原理：通过寻找一个最优超平面来划分不同类别的数据点，使得不同类别之间的间隔最大。
优点：在高维空间中有效，对于小样本数据也能取得较好效果。
6. 神经网络（Neural Networks）
用途：广泛用于分类、回归、图像识别、自然语言处理等多种任务。
原理：通过模拟人脑神经元的连接方式，构建多层网络结构来学习数据的复杂模式。
优点：能够处理非线性关系，具有强大的表示能力，尤其在大数据集上表现出色。
7. 梯度提升树（Gradient Boosting Trees）
用途：用于分类和回归问题。
原理：通过迭代地训练弱学习器（通常是决策树）并组合它们的输出来构建强学习器。
优点：通常比随机森林等集成方法具有更高的准确性，能够处理复杂的非线性关系。
8. K近邻算法（K-Nearest Neighbors, KNN）
用途：用于分类和回归问题。
原理：根据数据点之间的距离来寻找最近的K个邻居，并根据这些邻居的类别或值来预测目标数据点的类别或值。
优点：简单易懂，无需训练过程，适用于小数据集。
9. 朴素贝叶斯（Naive Bayes）
用途：用于分类问题，尤其在文本分类上表现出色。
原理：基于贝叶斯定理和特征条件独立假设来计算数据点属于每个类别的概率。
优点：计算效率高，对于高维数据也能取得较好效果，尤其适用于文本数据。
10. 聚类算法（如K-Means）
用途：用于无监督学习，将数据点划分为不同的簇。
原理：通过迭代地更新簇中心来最小化数据点到其所属簇中心的距离之和。
优点：简单易懂，计算效率高，适用于探索性数据分析。

模型选择建议
任务类型：根据任务是分类、回归还是聚类来选择相应的模型。
数据规模：对于大数据集，考虑使用神经网络或梯度提升树等能够处理高维数据和复杂关系的模型。
数据特性：如果数据具有线性关系，线性回归或逻辑回归可能足够；如果数据具有非线性关系，考虑使用决策树、随机森林或神经网络等模型。
计算资源：一些模型（如神经网络）需要大量的计算资源和时间来训练，而另一些模型（如朴素贝叶斯）则计算效率较高。
在实际应用中，通常需要根据具体任务和数据集的特点来选择最合适的模型，并通过交叉验证等方法来评估模型的性能。
```

## 3.2 交叉验证

```python
from sklearn.model_selection import StratifiedKFold
import numpy as np

# 假设我们有一个特征矩阵 X 和一个目标向量 y
X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]])
y = np.array([0, 0, 1, 1, 0, 1])  # 这是一个二分类问题的目标变量

# 定义交叉验证的参数
n_folds = 3  # 折数

# 初始化 StratifiedKFold 对象
sk = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=2019)

# 使用 StratifiedKFold 进行交叉验证
for train_index, test_index in sk.split(X, y):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    print("Train indices:", train_index, "Test indices:", test_index)
    print("X_train:", X_train, "X_test:", X_test)
    print("y_train:", y_train, "y_test:", y_test)
    print()  # 空行用于分隔不同的折


# 参数解释
# n_splits=n_folds：指定交叉验证的折数。在这个例子中，我们设置了 n_folds=3，意味着数据将被分成3个折，每次使用其中2个折作为训练集，1个折作为测试集，进行3次交叉验证。
# shuffle=True：在划分折之前是否对数据进行洗牌。洗牌有助于打破数据中的任何潜在顺序依赖，从而提高交叉验证的可靠性。
# random_state=2019：设置随机种子，以确保每次运行代码时都能得到相同的数据划分。这对于结果的可重复性非常重要。
```

```python
sk = StratifiedKFold(n_splits=11, shuffle=True, random_state=2019)
for train_, test_ in sk.split(x, y):
    # 注意是x.iloc[train_]，不是x[train_]
    # X_train训练集，X_test验证集，y_train就是预测值的训练集，y_test就是预测值的验证集
    X_train, X_test = x.iloc[train_], x.iloc[test_]
    y_train, y_test = y.iloc[train_], y.iloc[test_]
    
    
    ## 5.4 模型训练
    
    xgboost_clf.fit(X_train, y_train)
    
    ## 5.5 模型评估
    pred = xgboost_clf.predict_proba(X_test)[:,-1]
    
    print('auc:{}'.format(roc_auc_score(y_test, pred)))
```



## 3.3 超参数调优

```
超参数调优：在交叉验证的循环中，可以使用网格搜索或随机搜索等方法来调优模型的超参数。
```



# 5. 模型预测

```
fit_transform 和 transform的区别
关键区别
fit_transform：同时拟合编码器并转换数据。用于首次处理数据。
transform：仅转换数据，使用之前拟合过的编码器。用于处理新数据或测试数据。
注意事项
在使用 transform 之前，必须确保 LabelEncoder 已经通过 fit 或 fit_transform 方法拟合过数据。
如果你的数据中有新的类别标签（在拟合时未见过的），transform 方法会抛出一个错误，因为它不知道如何将这些新标签转换为数值编码。在这种情况下，你可能需要重新拟合编码器或使用其他策略来处理新类别。
```



```python
## 5.6 模型预测

### 5.6.1 概况预览
need_pred = pd.read_csv(f"{base_dir}\\test.csv")
# overViewAnalysis(need_pred)

### 5.6.2 空值填充
need_pred = need_pred.fillna({'authorities_contacted': 'no_contacted'})
need_pred['authorities_contacted'].isnull().sum()

### 5.6.3 特征编码
need_pred['incident_date_timestamp_days'] = date_to_timestamp_days(need_pred['incident_date']) 
need_pred['policy_bind_date_timestamp_days'] = date_to_timestamp_days(need_pred['policy_bind_date']) 

object_feature = list(need_pred.select_dtypes(include=['object']).columns)

# for col in object_feature:
#     # 正确的单独处理示例（避免重复使用同一encoder实例fit）：
#     onehot = OneHotEncoder(sparse_output=False, drop='first')
#     if col not in ['incident_date','policy_bind_date']:
#         # 注意：OneHotEncoder需要二维数组作为输入，因此我们需要对单个列进行reshape
#         need_pred[col] = onehot.fit_transform(need_pred[[col]])



for col in object_feature:
    if col not in ['incident_date','policy_bind_date']:
        # 注意：使用标签编码时,使用lb.transform,不用使用fit_transfomr再次进行拟合
        need_pred[col] = lb.transform(need_pred[col])
        
        
### 5.6.4 删除冗余数据
for col in ['policy_id','incident_date','policy_bind_date']: # ,
    del need_pred[col]

overViewAnalysis(need_pred)
```

```python
### 5.6.5 模型预测
need_pred_predvalue = xgboost_clf.predict_proba(need_pred)[:, -1]
need_pred_predvalue
# 将预测概率转换为0,1标签
final_pred = np.where(need_pred_predvalue >= 0.5, 1, 0)

### 5.6.6 结果输出
sub_df = pd.read_csv(f'{base_dir}\\submission.csv')
sub_df['fraud'] = final_pred
sub_df.to_csv(f'{base_dir}\\base_line.csv',index=False)
```



# 6. 总结

## 6.1 整体流程

- 了解数据概况
- 特征工程
  - 空值处理
  - 日期编码
  - 字符类型编码
- 数据清洗，去除冗余数据
- 模型训练
  - 模型选型
  - 交叉验证（训练集跟验证集的划分）
  - 超参数调优，网格搜索，随机搜索
- 模型预测



## 6.2 需要注意的地方

- 数据概况，注意缺失值，数据类型，重复值，也可以加入一些图形，以了解整体概况
- 数据编码，注意不同数据编码方式(独热编码，标签编码，目标编码，embedding编码，频率编码)的使用场景，但是这里，虽然我认为使用序数对日期编码更科学，但实际上，效果不如标签编码；注意使用独热编码时，需要进行二维数组的转换
- 交叉验证时，对于其几个参数的理解：n_splits这数，shuffle进行洗牌，random_state随机种子

```shell
# 对于随机种子的理解

随机种子与生活中的例子
例子：抽奖活动

想象你正在组织一个抽奖活动，你有一个装满奖券的箱子，每张奖券上都有一个编号，代表不同的奖品（比如一等奖、二等奖、三等奖等）。为了公平起见，你决定在抽奖前将奖券充分摇匀，然后随机抽取。

没有固定顺序（没有随机种子）：
每次抽奖前，你都只是简单地将奖券摇匀，然后随机抽取。
由于每次摇匀的程度和方式可能都不同，因此每次抽奖的结果（即抽出的奖券编号）都可能不同。
这就像在没有设置 random_state 的情况下进行随机操作，每次运行代码时结果都可能不同。
固定顺序（设置随机种子）：
现在，你决定在每次抽奖前都按照一个固定的方式摇匀奖券，比如先上下摇10次，再左右摇5次。
你还决定在每次抽奖前都从一个特定的位置开始抽取奖券，比如从箱子的左上角开始。
由于你每次都按照相同的方式摇匀奖券，并从相同的位置开始抽取，因此每次抽奖的结果（即抽出的奖券编号）都将是相同的（假设奖券没有被放回或替换）。
这就像设置了 random_state 一样，每次运行代码时，只要 random_state 相同，结果就会相同。
```

```
那为什么要有随机种子,直接给一个固定值不就行了吗
与固定值的区别
固定值不是随机性的替代：如果直接使用一个固定值来替代随机种子，那么我们就失去了模拟随机性的能力。每次运行程序时都会得到相同的结果，这在实际应用中往往是不希望的。
随机种子的动态性：随机种子允许我们在需要时“锁定”随机性（通过设置相同的种子），而在其他时候则利用真正的随机性（通过不设置种子或设置不同的种子）。这种动态性使得随机种子成为处理随机性问题的强大工具。
```

